{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "PyTorch is an open-source machine learning framework that is primarily used for developing and training deep learning models, It was\n",
    "developed by Facebooks Al Research Lab and released in 2016. PyTorch provides a flexible and dynamic approach to building neural networks,\n",
    "making it a popular choice among researchers and developers.\n",
    "The framework is built on a dynamic computational graph concept. which means that the graph is built and modified on-the-fly as the program\n",
    "runs. This allows for more intuitive and flexible model development. as you can use standard Python control flow statements and debug the\n",
    "model easily.\n",
    "PyTorch supports automatic differentiation, which enables efficient computation of gradients for training neural networks using\n",
    "backpropagation, It provides a rich set of tools and libraries for tasks such as data loading. model building. optimization. and evaluation.\n",
    "One of the key advantages of PyTorch is its support for GPU acceleration, allowing you to train models on GPUs to significantly speed up\n",
    "computations. It also has a large and active community. which means there are plenty Of resources. tutorials, and pretrained models available,\n",
    "PyTorch is often compared to TensorFlow, another popular deep learning framework. While TensorFlow focuses more on static computation\n",
    "graphs;PyTorch emphasizes dynamic computation graphs. This fundamental difference in design philosophy gives PyTorch an edge when it\n",
    "comes to flexibility and ease of use.\n",
    "Overall. PyTorch is Widely used in the research community and is garning popularity in industry applications as well. It provides a powerful and\n",
    "user-friendly platform for building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "- At its core, PyTorch is a library for processing tensors. A tensor is a number, vector, matrix, or any n-dimensional array. \n",
    "- Lets create a tensor with\n",
    "a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor with a single number\n",
    "t1 = torch.tensor(7.)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D tensor\n",
    "t3 = torch.tensor([[5., 6], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12, 13],\n",
       "         [13, 14, 15]],\n",
       "\n",
       "        [[15, 16, 17],\n",
       "         [17, 18, 19]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3D array\n",
    "t4 = torch.tensor([[[11, 12, 13],\n",
    "                    [13, 14, 15]],\n",
    "                   [[15, 16, 17],\n",
    "                    [17, 18, 19]]])\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensors can have any number of dimensions and different lengths along each dimension. \n",
    "- We can inspect the length along each dimension\n",
    "using the . shape property Of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t1)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t2)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t3)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11, 12, 13],\n",
      "         [13, 14, 15]],\n",
      "\n",
      "        [[15, 16, 17],\n",
      "         [17, 18, 19]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(t4)\n",
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m t5 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "t5 = torch.tensor([[5., 6, 11],\n",
    "                    [7, 8, 12],\n",
    "                    [9, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations and gradients\n",
    "We can combine tensors With the usual arithmetic operations Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, y isa tensor with the value 3 * 4 + = 17. \n",
    "- What makes PyTorch unique is that we can automatically compute the derivative of y w.r.t. the tensors that have requires_grad set to true ie. w and b.\n",
    "- This feature of PyTorch called autograd (automatic gradients).\n",
    "- To compute the derivatives. we can invoke the . backward method on our result y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print('dy/dx:', x.grad) \n",
    "print('dy/dw:', w.grad) # 3\n",
    "print('dy/db:', b.grad) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Functions\n",
    "\n",
    "Apart from arithmetic operations, the torch module also contains many functions for creating and manipulating tensors. Let's look at some\n",
    "examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 42],\n",
       "        [42, 42],\n",
       "        [42, 42]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6 = torch.full((3, 2), 42)\n",
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.],\n",
       "        [42., 42.],\n",
       "        [42., 42.],\n",
       "        [42., 42.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t7 = torch.cat((t3, t6))\n",
    "t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9589, -0.2794],\n",
       "        [ 0.6570,  0.9894],\n",
       "        [ 0.4121, -0.5440],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t8 = torch.sin(t7) \n",
    "t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9589, -0.2794],\n",
       "         [ 0.6570,  0.9894]],\n",
       "\n",
       "        [[ 0.4121, -0.5440],\n",
       "         [-0.9165, -0.9165]],\n",
       "\n",
       "        [[-0.9165, -0.9165],\n",
       "         [-0.9165, -0.9165]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t9 = t8.reshape(3, 2, 2)\n",
    "t9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperability with Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2], [3, 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, y.dtype    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a PyTorch tensor to a Numpy array using the . numpy method of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "input = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (apples, oranges)\n",
    "target = np.array([[56, 70],\n",
    "                    [81, 101],\n",
    "                    [119, 133],\n",
    "                    [22, 37],\n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(input)\n",
    "targets = torch.from_numpy(target)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0161, -1.4949, -0.4853],\n",
      "        [-0.6346,  0.5469,  1.6303]], requires_grad=True)\n",
      "tensor([-1.2032,  0.1876], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-121.0542,   60.6050],\n",
       "        [-162.3482,   94.9028],\n",
       "        [-228.2649,  112.8183],\n",
       "        [ -81.8000,   19.2927],\n",
       "        [-177.5721,  123.0218]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(actual, target):\n",
    "    diff = actual - target\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30151.6992, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss  = MSE(targets, preds)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-19045.7500, -22241.8164, -13361.9238],\n",
      "        [  -905.0275,   -849.1395,   -467.6789]])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-230.4079,   -9.8719])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# reset the gradients to zero\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30151.6992, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = MSE(targets, preds)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2065, -1.2725, -0.3517],\n",
      "        [-0.6256,  0.5554,  1.6350]], requires_grad=True)\n",
      "tensor([-1.2009,  0.1877], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20701.7910, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = MSE(targets, preds)  \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: tensor(20701.7910, grad_fn=<DivBackward0>)\n",
      "Epoch: 1 Loss: tensor(14328.9238, grad_fn=<DivBackward0>)\n",
      "Epoch: 2 Loss: tensor(10029.6953, grad_fn=<DivBackward0>)\n",
      "Epoch: 3 Loss: tensor(7127.9126, grad_fn=<DivBackward0>)\n",
      "Epoch: 4 Loss: tensor(5167.9092, grad_fn=<DivBackward0>)\n",
      "Epoch: 5 Loss: tensor(3842.6140, grad_fn=<DivBackward0>)\n",
      "Epoch: 6 Loss: tensor(2945.0977, grad_fn=<DivBackward0>)\n",
      "Epoch: 7 Loss: tensor(2335.9094, grad_fn=<DivBackward0>)\n",
      "Epoch: 8 Loss: tensor(1921.0756, grad_fn=<DivBackward0>)\n",
      "Epoch: 9 Loss: tensor(1637.2703, grad_fn=<DivBackward0>)\n",
      "Epoch: 10 Loss: tensor(1441.8156, grad_fn=<DivBackward0>)\n",
      "Epoch: 11 Loss: tensor(1305.9521, grad_fn=<DivBackward0>)\n",
      "Epoch: 12 Loss: tensor(1210.2986, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 Loss: tensor(1141.7937, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 Loss: tensor(1091.6343, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 Loss: tensor(1053.8883, grad_fn=<DivBackward0>)\n",
      "Epoch: 16 Loss: tensor(1024.5565, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 Loss: tensor(1000.9445, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 Loss: tensor(981.2349, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 Loss: tensor(964.2032, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 Loss: tensor(949.0228, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 Loss: tensor(935.1367, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 Loss: tensor(922.1689, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 Loss: tensor(909.8651, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 Loss: tensor(898.0535, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 Loss: tensor(886.6179, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 Loss: tensor(875.4795, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 Loss: tensor(864.5843, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 Loss: tensor(853.8955, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 Loss: tensor(843.3880, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 Loss: tensor(833.0443, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 Loss: tensor(822.8516, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 Loss: tensor(812.8013, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 Loss: tensor(802.8868, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 Loss: tensor(793.1037, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 Loss: tensor(783.4476, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 Loss: tensor(773.9158, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 Loss: tensor(764.5058, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 Loss: tensor(755.2152, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 Loss: tensor(746.0421, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 Loss: tensor(736.9848, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 Loss: tensor(728.0416, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 Loss: tensor(719.2111, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 Loss: tensor(710.4913, grad_fn=<DivBackward0>)\n",
      "Epoch: 44 Loss: tensor(701.8812, grad_fn=<DivBackward0>)\n",
      "Epoch: 45 Loss: tensor(693.3792, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 Loss: tensor(684.9841, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 Loss: tensor(676.6942, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 Loss: tensor(668.5084, grad_fn=<DivBackward0>)\n",
      "Epoch: 49 Loss: tensor(660.4257, grad_fn=<DivBackward0>)\n",
      "Epoch: 50 Loss: tensor(652.4440, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 Loss: tensor(644.5627, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 Loss: tensor(636.7801, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 Loss: tensor(629.0951, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 Loss: tensor(621.5066, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 Loss: tensor(614.0132, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 Loss: tensor(606.6138, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 Loss: tensor(599.3072, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 Loss: tensor(592.0922, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 Loss: tensor(584.9677, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 Loss: tensor(577.9324, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 Loss: tensor(570.9855, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 Loss: tensor(564.1256, grad_fn=<DivBackward0>)\n",
      "Epoch: 63 Loss: tensor(557.3515, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 Loss: tensor(550.6625, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 Loss: tensor(544.0574, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 Loss: tensor(537.5350, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 Loss: tensor(531.0942, grad_fn=<DivBackward0>)\n",
      "Epoch: 68 Loss: tensor(524.7343, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 Loss: tensor(518.4537, grad_fn=<DivBackward0>)\n",
      "Epoch: 70 Loss: tensor(512.2521, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 Loss: tensor(506.1280, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 Loss: tensor(500.0807, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 Loss: tensor(494.1092, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 Loss: tensor(488.2126, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 Loss: tensor(482.3894, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 Loss: tensor(476.6393, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 Loss: tensor(470.9612, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 Loss: tensor(465.3543, grad_fn=<DivBackward0>)\n",
      "Epoch: 79 Loss: tensor(459.8176, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 Loss: tensor(454.3499, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 Loss: tensor(448.9509, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 Loss: tensor(443.6193, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 Loss: tensor(438.3544, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 Loss: tensor(433.1557, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 Loss: tensor(428.0216, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 Loss: tensor(422.9519, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 Loss: tensor(417.9456, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 Loss: tensor(413.0019, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 Loss: tensor(408.1200, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 Loss: tensor(403.2992, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 Loss: tensor(398.5385, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 Loss: tensor(393.8376, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 Loss: tensor(389.1952, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 Loss: tensor(384.6108, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 Loss: tensor(380.0839, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 Loss: tensor(375.6134, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 Loss: tensor(371.1988, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 Loss: tensor(366.8393, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 Loss: tensor(362.5342, grad_fn=<DivBackward0>)\n",
      "Epoch: 100 Loss: tensor(358.2830, grad_fn=<DivBackward0>)\n",
      "Epoch: 101 Loss: tensor(354.0848, grad_fn=<DivBackward0>)\n",
      "Epoch: 102 Loss: tensor(349.9390, grad_fn=<DivBackward0>)\n",
      "Epoch: 103 Loss: tensor(345.8450, grad_fn=<DivBackward0>)\n",
      "Epoch: 104 Loss: tensor(341.8021, grad_fn=<DivBackward0>)\n",
      "Epoch: 105 Loss: tensor(337.8096, grad_fn=<DivBackward0>)\n",
      "Epoch: 106 Loss: tensor(333.8670, grad_fn=<DivBackward0>)\n",
      "Epoch: 107 Loss: tensor(329.9736, grad_fn=<DivBackward0>)\n",
      "Epoch: 108 Loss: tensor(326.1288, grad_fn=<DivBackward0>)\n",
      "Epoch: 109 Loss: tensor(322.3318, grad_fn=<DivBackward0>)\n",
      "Epoch: 110 Loss: tensor(318.5823, grad_fn=<DivBackward0>)\n",
      "Epoch: 111 Loss: tensor(314.8796, grad_fn=<DivBackward0>)\n",
      "Epoch: 112 Loss: tensor(311.2230, grad_fn=<DivBackward0>)\n",
      "Epoch: 113 Loss: tensor(307.6120, grad_fn=<DivBackward0>)\n",
      "Epoch: 114 Loss: tensor(304.0459, grad_fn=<DivBackward0>)\n",
      "Epoch: 115 Loss: tensor(300.5245, grad_fn=<DivBackward0>)\n",
      "Epoch: 116 Loss: tensor(297.0468, grad_fn=<DivBackward0>)\n",
      "Epoch: 117 Loss: tensor(293.6124, grad_fn=<DivBackward0>)\n",
      "Epoch: 118 Loss: tensor(290.2209, grad_fn=<DivBackward0>)\n",
      "Epoch: 119 Loss: tensor(286.8717, grad_fn=<DivBackward0>)\n",
      "Epoch: 120 Loss: tensor(283.5641, grad_fn=<DivBackward0>)\n",
      "Epoch: 121 Loss: tensor(280.2978, grad_fn=<DivBackward0>)\n",
      "Epoch: 122 Loss: tensor(277.0720, grad_fn=<DivBackward0>)\n",
      "Epoch: 123 Loss: tensor(273.8865, grad_fn=<DivBackward0>)\n",
      "Epoch: 124 Loss: tensor(270.7405, grad_fn=<DivBackward0>)\n",
      "Epoch: 125 Loss: tensor(267.6337, grad_fn=<DivBackward0>)\n",
      "Epoch: 126 Loss: tensor(264.5657, grad_fn=<DivBackward0>)\n",
      "Epoch: 127 Loss: tensor(261.5357, grad_fn=<DivBackward0>)\n",
      "Epoch: 128 Loss: tensor(258.5435, grad_fn=<DivBackward0>)\n",
      "Epoch: 129 Loss: tensor(255.5886, grad_fn=<DivBackward0>)\n",
      "Epoch: 130 Loss: tensor(252.6703, grad_fn=<DivBackward0>)\n",
      "Epoch: 131 Loss: tensor(249.7884, grad_fn=<DivBackward0>)\n",
      "Epoch: 132 Loss: tensor(246.9423, grad_fn=<DivBackward0>)\n",
      "Epoch: 133 Loss: tensor(244.1315, grad_fn=<DivBackward0>)\n",
      "Epoch: 134 Loss: tensor(241.3556, grad_fn=<DivBackward0>)\n",
      "Epoch: 135 Loss: tensor(238.6143, grad_fn=<DivBackward0>)\n",
      "Epoch: 136 Loss: tensor(235.9071, grad_fn=<DivBackward0>)\n",
      "Epoch: 137 Loss: tensor(233.2333, grad_fn=<DivBackward0>)\n",
      "Epoch: 138 Loss: tensor(230.5930, grad_fn=<DivBackward0>)\n",
      "Epoch: 139 Loss: tensor(227.9853, grad_fn=<DivBackward0>)\n",
      "Epoch: 140 Loss: tensor(225.4100, grad_fn=<DivBackward0>)\n",
      "Epoch: 141 Loss: tensor(222.8666, grad_fn=<DivBackward0>)\n",
      "Epoch: 142 Loss: tensor(220.3548, grad_fn=<DivBackward0>)\n",
      "Epoch: 143 Loss: tensor(217.8743, grad_fn=<DivBackward0>)\n",
      "Epoch: 144 Loss: tensor(215.4244, grad_fn=<DivBackward0>)\n",
      "Epoch: 145 Loss: tensor(213.0050, grad_fn=<DivBackward0>)\n",
      "Epoch: 146 Loss: tensor(210.6156, grad_fn=<DivBackward0>)\n",
      "Epoch: 147 Loss: tensor(208.2558, grad_fn=<DivBackward0>)\n",
      "Epoch: 148 Loss: tensor(205.9252, grad_fn=<DivBackward0>)\n",
      "Epoch: 149 Loss: tensor(203.6235, grad_fn=<DivBackward0>)\n",
      "Epoch: 150 Loss: tensor(201.3504, grad_fn=<DivBackward0>)\n",
      "Epoch: 151 Loss: tensor(199.1054, grad_fn=<DivBackward0>)\n",
      "Epoch: 152 Loss: tensor(196.8882, grad_fn=<DivBackward0>)\n",
      "Epoch: 153 Loss: tensor(194.6984, grad_fn=<DivBackward0>)\n",
      "Epoch: 154 Loss: tensor(192.5357, grad_fn=<DivBackward0>)\n",
      "Epoch: 155 Loss: tensor(190.3999, grad_fn=<DivBackward0>)\n",
      "Epoch: 156 Loss: tensor(188.2904, grad_fn=<DivBackward0>)\n",
      "Epoch: 157 Loss: tensor(186.2072, grad_fn=<DivBackward0>)\n",
      "Epoch: 158 Loss: tensor(184.1495, grad_fn=<DivBackward0>)\n",
      "Epoch: 159 Loss: tensor(182.1174, grad_fn=<DivBackward0>)\n",
      "Epoch: 160 Loss: tensor(180.1104, grad_fn=<DivBackward0>)\n",
      "Epoch: 161 Loss: tensor(178.1282, grad_fn=<DivBackward0>)\n",
      "Epoch: 162 Loss: tensor(176.1706, grad_fn=<DivBackward0>)\n",
      "Epoch: 163 Loss: tensor(174.2371, grad_fn=<DivBackward0>)\n",
      "Epoch: 164 Loss: tensor(172.3275, grad_fn=<DivBackward0>)\n",
      "Epoch: 165 Loss: tensor(170.4414, grad_fn=<DivBackward0>)\n",
      "Epoch: 166 Loss: tensor(168.5786, grad_fn=<DivBackward0>)\n",
      "Epoch: 167 Loss: tensor(166.7389, grad_fn=<DivBackward0>)\n",
      "Epoch: 168 Loss: tensor(164.9218, grad_fn=<DivBackward0>)\n",
      "Epoch: 169 Loss: tensor(163.1272, grad_fn=<DivBackward0>)\n",
      "Epoch: 170 Loss: tensor(161.3547, grad_fn=<DivBackward0>)\n",
      "Epoch: 171 Loss: tensor(159.6041, grad_fn=<DivBackward0>)\n",
      "Epoch: 172 Loss: tensor(157.8750, grad_fn=<DivBackward0>)\n",
      "Epoch: 173 Loss: tensor(156.1674, grad_fn=<DivBackward0>)\n",
      "Epoch: 174 Loss: tensor(154.4806, grad_fn=<DivBackward0>)\n",
      "Epoch: 175 Loss: tensor(152.8148, grad_fn=<DivBackward0>)\n",
      "Epoch: 176 Loss: tensor(151.1694, grad_fn=<DivBackward0>)\n",
      "Epoch: 177 Loss: tensor(149.5441, grad_fn=<DivBackward0>)\n",
      "Epoch: 178 Loss: tensor(147.9390, grad_fn=<DivBackward0>)\n",
      "Epoch: 179 Loss: tensor(146.3537, grad_fn=<DivBackward0>)\n",
      "Epoch: 180 Loss: tensor(144.7877, grad_fn=<DivBackward0>)\n",
      "Epoch: 181 Loss: tensor(143.2411, grad_fn=<DivBackward0>)\n",
      "Epoch: 182 Loss: tensor(141.7135, grad_fn=<DivBackward0>)\n",
      "Epoch: 183 Loss: tensor(140.2047, grad_fn=<DivBackward0>)\n",
      "Epoch: 184 Loss: tensor(138.7144, grad_fn=<DivBackward0>)\n",
      "Epoch: 185 Loss: tensor(137.2425, grad_fn=<DivBackward0>)\n",
      "Epoch: 186 Loss: tensor(135.7885, grad_fn=<DivBackward0>)\n",
      "Epoch: 187 Loss: tensor(134.3524, grad_fn=<DivBackward0>)\n",
      "Epoch: 188 Loss: tensor(132.9341, grad_fn=<DivBackward0>)\n",
      "Epoch: 189 Loss: tensor(131.5330, grad_fn=<DivBackward0>)\n",
      "Epoch: 190 Loss: tensor(130.1492, grad_fn=<DivBackward0>)\n",
      "Epoch: 191 Loss: tensor(128.7823, grad_fn=<DivBackward0>)\n",
      "Epoch: 192 Loss: tensor(127.4322, grad_fn=<DivBackward0>)\n",
      "Epoch: 193 Loss: tensor(126.0986, grad_fn=<DivBackward0>)\n",
      "Epoch: 194 Loss: tensor(124.7814, grad_fn=<DivBackward0>)\n",
      "Epoch: 195 Loss: tensor(123.4803, grad_fn=<DivBackward0>)\n",
      "Epoch: 196 Loss: tensor(122.1952, grad_fn=<DivBackward0>)\n",
      "Epoch: 197 Loss: tensor(120.9257, grad_fn=<DivBackward0>)\n",
      "Epoch: 198 Loss: tensor(119.6718, grad_fn=<DivBackward0>)\n",
      "Epoch: 199 Loss: tensor(118.4332, grad_fn=<DivBackward0>)\n",
      "Epoch: 200 Loss: tensor(117.2098, grad_fn=<DivBackward0>)\n",
      "Epoch: 201 Loss: tensor(116.0013, grad_fn=<DivBackward0>)\n",
      "Epoch: 202 Loss: tensor(114.8076, grad_fn=<DivBackward0>)\n",
      "Epoch: 203 Loss: tensor(113.6285, grad_fn=<DivBackward0>)\n",
      "Epoch: 204 Loss: tensor(112.4637, grad_fn=<DivBackward0>)\n",
      "Epoch: 205 Loss: tensor(111.3132, grad_fn=<DivBackward0>)\n",
      "Epoch: 206 Loss: tensor(110.1768, grad_fn=<DivBackward0>)\n",
      "Epoch: 207 Loss: tensor(109.0542, grad_fn=<DivBackward0>)\n",
      "Epoch: 208 Loss: tensor(107.9452, grad_fn=<DivBackward0>)\n",
      "Epoch: 209 Loss: tensor(106.8498, grad_fn=<DivBackward0>)\n",
      "Epoch: 210 Loss: tensor(105.7677, grad_fn=<DivBackward0>)\n",
      "Epoch: 211 Loss: tensor(104.6988, grad_fn=<DivBackward0>)\n",
      "Epoch: 212 Loss: tensor(103.6430, grad_fn=<DivBackward0>)\n",
      "Epoch: 213 Loss: tensor(102.5999, grad_fn=<DivBackward0>)\n",
      "Epoch: 214 Loss: tensor(101.5696, grad_fn=<DivBackward0>)\n",
      "Epoch: 215 Loss: tensor(100.5517, grad_fn=<DivBackward0>)\n",
      "Epoch: 216 Loss: tensor(99.5463, grad_fn=<DivBackward0>)\n",
      "Epoch: 217 Loss: tensor(98.5530, grad_fn=<DivBackward0>)\n",
      "Epoch: 218 Loss: tensor(97.5718, grad_fn=<DivBackward0>)\n",
      "Epoch: 219 Loss: tensor(96.6026, grad_fn=<DivBackward0>)\n",
      "Epoch: 220 Loss: tensor(95.6451, grad_fn=<DivBackward0>)\n",
      "Epoch: 221 Loss: tensor(94.6992, grad_fn=<DivBackward0>)\n",
      "Epoch: 222 Loss: tensor(93.7648, grad_fn=<DivBackward0>)\n",
      "Epoch: 223 Loss: tensor(92.8416, grad_fn=<DivBackward0>)\n",
      "Epoch: 224 Loss: tensor(91.9298, grad_fn=<DivBackward0>)\n",
      "Epoch: 225 Loss: tensor(91.0289, grad_fn=<DivBackward0>)\n",
      "Epoch: 226 Loss: tensor(90.1389, grad_fn=<DivBackward0>)\n",
      "Epoch: 227 Loss: tensor(89.2597, grad_fn=<DivBackward0>)\n",
      "Epoch: 228 Loss: tensor(88.3912, grad_fn=<DivBackward0>)\n",
      "Epoch: 229 Loss: tensor(87.5331, grad_fn=<DivBackward0>)\n",
      "Epoch: 230 Loss: tensor(86.6853, grad_fn=<DivBackward0>)\n",
      "Epoch: 231 Loss: tensor(85.8478, grad_fn=<DivBackward0>)\n",
      "Epoch: 232 Loss: tensor(85.0205, grad_fn=<DivBackward0>)\n",
      "Epoch: 233 Loss: tensor(84.2031, grad_fn=<DivBackward0>)\n",
      "Epoch: 234 Loss: tensor(83.3955, grad_fn=<DivBackward0>)\n",
      "Epoch: 235 Loss: tensor(82.5977, grad_fn=<DivBackward0>)\n",
      "Epoch: 236 Loss: tensor(81.8095, grad_fn=<DivBackward0>)\n",
      "Epoch: 237 Loss: tensor(81.0309, grad_fn=<DivBackward0>)\n",
      "Epoch: 238 Loss: tensor(80.2616, grad_fn=<DivBackward0>)\n",
      "Epoch: 239 Loss: tensor(79.5015, grad_fn=<DivBackward0>)\n",
      "Epoch: 240 Loss: tensor(78.7505, grad_fn=<DivBackward0>)\n",
      "Epoch: 241 Loss: tensor(78.0086, grad_fn=<DivBackward0>)\n",
      "Epoch: 242 Loss: tensor(77.2756, grad_fn=<DivBackward0>)\n",
      "Epoch: 243 Loss: tensor(76.5514, grad_fn=<DivBackward0>)\n",
      "Epoch: 244 Loss: tensor(75.8359, grad_fn=<DivBackward0>)\n",
      "Epoch: 245 Loss: tensor(75.1289, grad_fn=<DivBackward0>)\n",
      "Epoch: 246 Loss: tensor(74.4304, grad_fn=<DivBackward0>)\n",
      "Epoch: 247 Loss: tensor(73.7403, grad_fn=<DivBackward0>)\n",
      "Epoch: 248 Loss: tensor(73.0585, grad_fn=<DivBackward0>)\n",
      "Epoch: 249 Loss: tensor(72.3848, grad_fn=<DivBackward0>)\n",
      "Epoch: 250 Loss: tensor(71.7191, grad_fn=<DivBackward0>)\n",
      "Epoch: 251 Loss: tensor(71.0615, grad_fn=<DivBackward0>)\n",
      "Epoch: 252 Loss: tensor(70.4116, grad_fn=<DivBackward0>)\n",
      "Epoch: 253 Loss: tensor(69.7695, grad_fn=<DivBackward0>)\n",
      "Epoch: 254 Loss: tensor(69.1351, grad_fn=<DivBackward0>)\n",
      "Epoch: 255 Loss: tensor(68.5082, grad_fn=<DivBackward0>)\n",
      "Epoch: 256 Loss: tensor(67.8889, grad_fn=<DivBackward0>)\n",
      "Epoch: 257 Loss: tensor(67.2769, grad_fn=<DivBackward0>)\n",
      "Epoch: 258 Loss: tensor(66.6721, grad_fn=<DivBackward0>)\n",
      "Epoch: 259 Loss: tensor(66.0746, grad_fn=<DivBackward0>)\n",
      "Epoch: 260 Loss: tensor(65.4842, grad_fn=<DivBackward0>)\n",
      "Epoch: 261 Loss: tensor(64.9007, grad_fn=<DivBackward0>)\n",
      "Epoch: 262 Loss: tensor(64.3242, grad_fn=<DivBackward0>)\n",
      "Epoch: 263 Loss: tensor(63.7546, grad_fn=<DivBackward0>)\n",
      "Epoch: 264 Loss: tensor(63.1917, grad_fn=<DivBackward0>)\n",
      "Epoch: 265 Loss: tensor(62.6354, grad_fn=<DivBackward0>)\n",
      "Epoch: 266 Loss: tensor(62.0858, grad_fn=<DivBackward0>)\n",
      "Epoch: 267 Loss: tensor(61.5427, grad_fn=<DivBackward0>)\n",
      "Epoch: 268 Loss: tensor(61.0059, grad_fn=<DivBackward0>)\n",
      "Epoch: 269 Loss: tensor(60.4755, grad_fn=<DivBackward0>)\n",
      "Epoch: 270 Loss: tensor(59.9514, grad_fn=<DivBackward0>)\n",
      "Epoch: 271 Loss: tensor(59.4334, grad_fn=<DivBackward0>)\n",
      "Epoch: 272 Loss: tensor(58.9216, grad_fn=<DivBackward0>)\n",
      "Epoch: 273 Loss: tensor(58.4158, grad_fn=<DivBackward0>)\n",
      "Epoch: 274 Loss: tensor(57.9160, grad_fn=<DivBackward0>)\n",
      "Epoch: 275 Loss: tensor(57.4220, grad_fn=<DivBackward0>)\n",
      "Epoch: 276 Loss: tensor(56.9338, grad_fn=<DivBackward0>)\n",
      "Epoch: 277 Loss: tensor(56.4514, grad_fn=<DivBackward0>)\n",
      "Epoch: 278 Loss: tensor(55.9745, grad_fn=<DivBackward0>)\n",
      "Epoch: 279 Loss: tensor(55.5034, grad_fn=<DivBackward0>)\n",
      "Epoch: 280 Loss: tensor(55.0378, grad_fn=<DivBackward0>)\n",
      "Epoch: 281 Loss: tensor(54.5776, grad_fn=<DivBackward0>)\n",
      "Epoch: 282 Loss: tensor(54.1227, grad_fn=<DivBackward0>)\n",
      "Epoch: 283 Loss: tensor(53.6732, grad_fn=<DivBackward0>)\n",
      "Epoch: 284 Loss: tensor(53.2290, grad_fn=<DivBackward0>)\n",
      "Epoch: 285 Loss: tensor(52.7899, grad_fn=<DivBackward0>)\n",
      "Epoch: 286 Loss: tensor(52.3560, grad_fn=<DivBackward0>)\n",
      "Epoch: 287 Loss: tensor(51.9271, grad_fn=<DivBackward0>)\n",
      "Epoch: 288 Loss: tensor(51.5032, grad_fn=<DivBackward0>)\n",
      "Epoch: 289 Loss: tensor(51.0843, grad_fn=<DivBackward0>)\n",
      "Epoch: 290 Loss: tensor(50.6702, grad_fn=<DivBackward0>)\n",
      "Epoch: 291 Loss: tensor(50.2609, grad_fn=<DivBackward0>)\n",
      "Epoch: 292 Loss: tensor(49.8564, grad_fn=<DivBackward0>)\n",
      "Epoch: 293 Loss: tensor(49.4565, grad_fn=<DivBackward0>)\n",
      "Epoch: 294 Loss: tensor(49.0613, grad_fn=<DivBackward0>)\n",
      "Epoch: 295 Loss: tensor(48.6707, grad_fn=<DivBackward0>)\n",
      "Epoch: 296 Loss: tensor(48.2846, grad_fn=<DivBackward0>)\n",
      "Epoch: 297 Loss: tensor(47.9029, grad_fn=<DivBackward0>)\n",
      "Epoch: 298 Loss: tensor(47.5257, grad_fn=<DivBackward0>)\n",
      "Epoch: 299 Loss: tensor(47.1528, grad_fn=<DivBackward0>)\n",
      "Epoch: 300 Loss: tensor(46.7842, grad_fn=<DivBackward0>)\n",
      "Epoch: 301 Loss: tensor(46.4198, grad_fn=<DivBackward0>)\n",
      "Epoch: 302 Loss: tensor(46.0596, grad_fn=<DivBackward0>)\n",
      "Epoch: 303 Loss: tensor(45.7037, grad_fn=<DivBackward0>)\n",
      "Epoch: 304 Loss: tensor(45.3517, grad_fn=<DivBackward0>)\n",
      "Epoch: 305 Loss: tensor(45.0038, grad_fn=<DivBackward0>)\n",
      "Epoch: 306 Loss: tensor(44.6599, grad_fn=<DivBackward0>)\n",
      "Epoch: 307 Loss: tensor(44.3199, grad_fn=<DivBackward0>)\n",
      "Epoch: 308 Loss: tensor(43.9838, grad_fn=<DivBackward0>)\n",
      "Epoch: 309 Loss: tensor(43.6515, grad_fn=<DivBackward0>)\n",
      "Epoch: 310 Loss: tensor(43.3231, grad_fn=<DivBackward0>)\n",
      "Epoch: 311 Loss: tensor(42.9984, grad_fn=<DivBackward0>)\n",
      "Epoch: 312 Loss: tensor(42.6774, grad_fn=<DivBackward0>)\n",
      "Epoch: 313 Loss: tensor(42.3600, grad_fn=<DivBackward0>)\n",
      "Epoch: 314 Loss: tensor(42.0462, grad_fn=<DivBackward0>)\n",
      "Epoch: 315 Loss: tensor(41.7360, grad_fn=<DivBackward0>)\n",
      "Epoch: 316 Loss: tensor(41.4293, grad_fn=<DivBackward0>)\n",
      "Epoch: 317 Loss: tensor(41.1261, grad_fn=<DivBackward0>)\n",
      "Epoch: 318 Loss: tensor(40.8263, grad_fn=<DivBackward0>)\n",
      "Epoch: 319 Loss: tensor(40.5299, grad_fn=<DivBackward0>)\n",
      "Epoch: 320 Loss: tensor(40.2369, grad_fn=<DivBackward0>)\n",
      "Epoch: 321 Loss: tensor(39.9471, grad_fn=<DivBackward0>)\n",
      "Epoch: 322 Loss: tensor(39.6607, grad_fn=<DivBackward0>)\n",
      "Epoch: 323 Loss: tensor(39.3774, grad_fn=<DivBackward0>)\n",
      "Epoch: 324 Loss: tensor(39.0973, grad_fn=<DivBackward0>)\n",
      "Epoch: 325 Loss: tensor(38.8204, grad_fn=<DivBackward0>)\n",
      "Epoch: 326 Loss: tensor(38.5466, grad_fn=<DivBackward0>)\n",
      "Epoch: 327 Loss: tensor(38.2758, grad_fn=<DivBackward0>)\n",
      "Epoch: 328 Loss: tensor(38.0080, grad_fn=<DivBackward0>)\n",
      "Epoch: 329 Loss: tensor(37.7433, grad_fn=<DivBackward0>)\n",
      "Epoch: 330 Loss: tensor(37.4815, grad_fn=<DivBackward0>)\n",
      "Epoch: 331 Loss: tensor(37.2227, grad_fn=<DivBackward0>)\n",
      "Epoch: 332 Loss: tensor(36.9667, grad_fn=<DivBackward0>)\n",
      "Epoch: 333 Loss: tensor(36.7135, grad_fn=<DivBackward0>)\n",
      "Epoch: 334 Loss: tensor(36.4631, grad_fn=<DivBackward0>)\n",
      "Epoch: 335 Loss: tensor(36.2156, grad_fn=<DivBackward0>)\n",
      "Epoch: 336 Loss: tensor(35.9708, grad_fn=<DivBackward0>)\n",
      "Epoch: 337 Loss: tensor(35.7287, grad_fn=<DivBackward0>)\n",
      "Epoch: 338 Loss: tensor(35.4892, grad_fn=<DivBackward0>)\n",
      "Epoch: 339 Loss: tensor(35.2524, grad_fn=<DivBackward0>)\n",
      "Epoch: 340 Loss: tensor(35.0182, grad_fn=<DivBackward0>)\n",
      "Epoch: 341 Loss: tensor(34.7866, grad_fn=<DivBackward0>)\n",
      "Epoch: 342 Loss: tensor(34.5575, grad_fn=<DivBackward0>)\n",
      "Epoch: 343 Loss: tensor(34.3310, grad_fn=<DivBackward0>)\n",
      "Epoch: 344 Loss: tensor(34.1069, grad_fn=<DivBackward0>)\n",
      "Epoch: 345 Loss: tensor(33.8853, grad_fn=<DivBackward0>)\n",
      "Epoch: 346 Loss: tensor(33.6661, grad_fn=<DivBackward0>)\n",
      "Epoch: 347 Loss: tensor(33.4493, grad_fn=<DivBackward0>)\n",
      "Epoch: 348 Loss: tensor(33.2348, grad_fn=<DivBackward0>)\n",
      "Epoch: 349 Loss: tensor(33.0227, grad_fn=<DivBackward0>)\n",
      "Epoch: 350 Loss: tensor(32.8128, grad_fn=<DivBackward0>)\n",
      "Epoch: 351 Loss: tensor(32.6053, grad_fn=<DivBackward0>)\n",
      "Epoch: 352 Loss: tensor(32.4000, grad_fn=<DivBackward0>)\n",
      "Epoch: 353 Loss: tensor(32.1969, grad_fn=<DivBackward0>)\n",
      "Epoch: 354 Loss: tensor(31.9960, grad_fn=<DivBackward0>)\n",
      "Epoch: 355 Loss: tensor(31.7973, grad_fn=<DivBackward0>)\n",
      "Epoch: 356 Loss: tensor(31.6007, grad_fn=<DivBackward0>)\n",
      "Epoch: 357 Loss: tensor(31.4063, grad_fn=<DivBackward0>)\n",
      "Epoch: 358 Loss: tensor(31.2139, grad_fn=<DivBackward0>)\n",
      "Epoch: 359 Loss: tensor(31.0235, grad_fn=<DivBackward0>)\n",
      "Epoch: 360 Loss: tensor(30.8353, grad_fn=<DivBackward0>)\n",
      "Epoch: 361 Loss: tensor(30.6490, grad_fn=<DivBackward0>)\n",
      "Epoch: 362 Loss: tensor(30.4647, grad_fn=<DivBackward0>)\n",
      "Epoch: 363 Loss: tensor(30.2823, grad_fn=<DivBackward0>)\n",
      "Epoch: 364 Loss: tensor(30.1019, grad_fn=<DivBackward0>)\n",
      "Epoch: 365 Loss: tensor(29.9234, grad_fn=<DivBackward0>)\n",
      "Epoch: 366 Loss: tensor(29.7469, grad_fn=<DivBackward0>)\n",
      "Epoch: 367 Loss: tensor(29.5721, grad_fn=<DivBackward0>)\n",
      "Epoch: 368 Loss: tensor(29.3992, grad_fn=<DivBackward0>)\n",
      "Epoch: 369 Loss: tensor(29.2282, grad_fn=<DivBackward0>)\n",
      "Epoch: 370 Loss: tensor(29.0589, grad_fn=<DivBackward0>)\n",
      "Epoch: 371 Loss: tensor(28.8914, grad_fn=<DivBackward0>)\n",
      "Epoch: 372 Loss: tensor(28.7257, grad_fn=<DivBackward0>)\n",
      "Epoch: 373 Loss: tensor(28.5617, grad_fn=<DivBackward0>)\n",
      "Epoch: 374 Loss: tensor(28.3994, grad_fn=<DivBackward0>)\n",
      "Epoch: 375 Loss: tensor(28.2388, grad_fn=<DivBackward0>)\n",
      "Epoch: 376 Loss: tensor(28.0798, grad_fn=<DivBackward0>)\n",
      "Epoch: 377 Loss: tensor(27.9226, grad_fn=<DivBackward0>)\n",
      "Epoch: 378 Loss: tensor(27.7669, grad_fn=<DivBackward0>)\n",
      "Epoch: 379 Loss: tensor(27.6129, grad_fn=<DivBackward0>)\n",
      "Epoch: 380 Loss: tensor(27.4604, grad_fn=<DivBackward0>)\n",
      "Epoch: 381 Loss: tensor(27.3095, grad_fn=<DivBackward0>)\n",
      "Epoch: 382 Loss: tensor(27.1602, grad_fn=<DivBackward0>)\n",
      "Epoch: 383 Loss: tensor(27.0124, grad_fn=<DivBackward0>)\n",
      "Epoch: 384 Loss: tensor(26.8661, grad_fn=<DivBackward0>)\n",
      "Epoch: 385 Loss: tensor(26.7214, grad_fn=<DivBackward0>)\n",
      "Epoch: 386 Loss: tensor(26.5781, grad_fn=<DivBackward0>)\n",
      "Epoch: 387 Loss: tensor(26.4362, grad_fn=<DivBackward0>)\n",
      "Epoch: 388 Loss: tensor(26.2958, grad_fn=<DivBackward0>)\n",
      "Epoch: 389 Loss: tensor(26.1568, grad_fn=<DivBackward0>)\n",
      "Epoch: 390 Loss: tensor(26.0192, grad_fn=<DivBackward0>)\n",
      "Epoch: 391 Loss: tensor(25.8830, grad_fn=<DivBackward0>)\n",
      "Epoch: 392 Loss: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.7482, grad_fn=<DivBackward0>)\n",
      "Epoch: 393 Loss: tensor(25.6148, grad_fn=<DivBackward0>)\n",
      "Epoch: 394 Loss: tensor(25.4827, grad_fn=<DivBackward0>)\n",
      "Epoch: 395 Loss: tensor(25.3519, grad_fn=<DivBackward0>)\n",
      "Epoch: 396 Loss: tensor(25.2224, grad_fn=<DivBackward0>)\n",
      "Epoch: 397 Loss: tensor(25.0942, grad_fn=<DivBackward0>)\n",
      "Epoch: 398 Loss: tensor(24.9673, grad_fn=<DivBackward0>)\n",
      "Epoch: 399 Loss: tensor(24.8416, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):\n",
    "    preds = model(inputs)\n",
    "    loss = MSE(targets, preds)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(\"Epoch:\", i, \"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.8416, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 58.3406,  69.7955],\n",
       "         [ 81.7443, 104.4120],\n",
       "         [117.8515, 125.2747],\n",
       "         [ 28.9534,  34.2601],\n",
       "         [ 96.2854, 127.2921]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 13 16:30:02 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.76                 Driver Version: 551.76         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P3             12W /   30W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Adjust weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.587989  [    0/60000]\n",
      "loss: 1.620953  [ 6400/60000]\n",
      "loss: 1.532960  [12800/60000]\n",
      "loss: 1.653096  [19200/60000]\n",
      "loss: 1.403436  [25600/60000]\n",
      "loss: 1.470638  [32000/60000]\n",
      "loss: 1.481000  [38400/60000]\n",
      "loss: 1.456102  [44800/60000]\n",
      "loss: 1.430298  [51200/60000]\n",
      "loss: 1.409010  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 0.023358 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.483725  [    0/60000]\n",
      "loss: 1.532291  [ 6400/60000]\n",
      "loss: 1.431131  [12800/60000]\n",
      "loss: 1.579913  [19200/60000]\n",
      "loss: 1.310635  [25600/60000]\n",
      "loss: 1.393952  [32000/60000]\n",
      "loss: 1.400206  [38400/60000]\n",
      "loss: 1.377579  [44800/60000]\n",
      "loss: 1.354373  [51200/60000]\n",
      "loss: 1.347777  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.022282 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.408212  [    0/60000]\n",
      "loss: 1.468591  [ 6400/60000]\n",
      "loss: 1.351567  [12800/60000]\n",
      "loss: 1.524507  [19200/60000]\n",
      "loss: 1.246955  [25600/60000]\n",
      "loss: 1.337704  [32000/60000]\n",
      "loss: 1.345221  [38400/60000]\n",
      "loss: 1.319432  [44800/60000]\n",
      "loss: 1.298488  [51200/60000]\n",
      "loss: 1.304700  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 0.021491 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.352855  [    0/60000]\n",
      "loss: 1.420526  [ 6400/60000]\n",
      "loss: 1.288044  [12800/60000]\n",
      "loss: 1.480652  [19200/60000]\n",
      "loss: 1.201580  [25600/60000]\n",
      "loss: 1.295236  [32000/60000]\n",
      "loss: 1.306482  [38400/60000]\n",
      "loss: 1.276329  [44800/60000]\n",
      "loss: 1.255422  [51200/60000]\n",
      "loss: 1.273620  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.2%, Avg loss: 0.020888 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.309864  [    0/60000]\n",
      "loss: 1.380562  [ 6400/60000]\n",
      "loss: 1.237535  [12800/60000]\n",
      "loss: 1.444075  [19200/60000]\n",
      "loss: 1.167640  [25600/60000]\n",
      "loss: 1.262099  [32000/60000]\n",
      "loss: 1.277074  [38400/60000]\n",
      "loss: 1.241498  [44800/60000]\n",
      "loss: 1.221545  [51200/60000]\n",
      "loss: 1.249795  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.020412 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.heatmap(test_data[0][0][0], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
